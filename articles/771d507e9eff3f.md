---
title: "Deep Learningè³‡æ ¼è©¦é¨“ æ·±å±¤å­¦ç¿’ æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ "
emoji: "ğŸ’¨"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["Eè³‡æ ¼"]
published: true
---

# ã¯ã˜ã‚ã«

æ—¥æœ¬ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°å”ä¼šã® Deep Learning è³‡æ ¼è©¦é¨“ï¼ˆE è³‡æ ¼ï¼‰ã®å—é¨“ã«å‘ã‘ã¦ã€èª¿ã¹ãŸå†…å®¹ã‚’ã¾ã¨ã‚ã¦ã„ãã¾ã™ã€‚

# å‹¾é…é™ä¸‹æ³•

## ç‰¹å¾´

- ç›®çš„é–¢æ•°ãŒæœ€å°ã¨ãªã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‹¾é…è¡¨ã«ã‚ˆã£ã¦æ¢ã™ã€‚
- ç›®çš„é–¢æ•°ãŒå„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚ˆã£ã¦å¾®åˆ†ã§ãã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

- ãƒ¡ãƒªãƒƒãƒˆ
  - ãƒ‡ãƒ¼ã‚¿ãŒå†—â»‘ãªå ´åˆã®è¨ˆç®—ã‚³ã‚¹ãƒˆã®è»½æ¸›
  - æœ›ã¾ãªã„å±€æ‰€æ¥µå°è§£ã«åæŸã™ã‚‹ãƒªã‚¹ã‚¯ã®è»½æ¸›
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ãŒã§ãã‚‹

## è¨ˆç®—å¼

å­¦ç¿’ç‡ï¼š$\varepsilon$

$$
\begin{aligned}
w^{(t+1)} = w^{(t)} - \varepsilon \nabla E
\end{aligned}
$$

# ç¢ºç‡çš„å‹¾é…é™ä¸‹æ³•ï¼ˆSDGï¼‰

- ç„¡ä½œç‚ºã«é¸ã³å‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦è¡Œã†å‹¾é…é™ä¸‹æ³•ã§ã‚ã‚‹ã€‚

## ã‚³ãƒ¼ãƒ‰

```
lr            å­¦ç¿’ç‡ï¼ˆlearning rateï¼‰ï¼ˆå°æ•°ï¼‰
params        å­¦ç¿’ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆè¡Œåˆ—ï¼‰
grads         ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é…ï¼ˆè¡Œåˆ—ï¼‰
```

```python:python
class SGD:

    """ç¢ºç‡çš„å‹¾é…é™ä¸‹æ³•ï¼ˆStochastic Gradient Descentï¼‰"""

    def __init__(self, lr=0.01):
        # å­¦ç¿’ç‡
        self.lr = lr

    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]
```

# ãƒŸãƒ‹ãƒãƒƒãƒå‹¾é…é™ä¸‹æ³•

- ç„¡ä½œç‚ºã«é¸ã³å‡ºã•ã‚ŒãŸè¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦è¡Œã†å‹¾é…é™ä¸‹æ³•ã§ã‚ã‚‹ã€‚

# ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ 

## ç‰¹å¾´

- èª¤å·®ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å¾®åˆ†ã—ãŸã‚‚ã®ã¨å­¦ç¿’ç‡ã®ç©ã‚’æ¸›ç®—ã—ãŸå¾Œã€ç¾åœ¨ã®é‡ã¿ã«å‰å›ã®é‡ã¿ã‚’æ¸›ç®—ã—ãŸå€¤ã¨æ…£æ€§ã®ç©ã‚’åŠ ç®—ã™ã‚‹

- ãƒ¡ãƒªãƒƒãƒˆ
  - å±€æ‰€çš„æœ€é©è§£ã«ã¯ãªã‚‰ãšã€å¤§åŸŸçš„æœ€é©è§£ã¨ãªã‚‹ã€‚
  - è°·é–“ã«ã¤ã„ã¦ã‹ã‚‰æœ€ã‚‚ä½ã„ä½ç½®(æœ€é©å€¤)ã«ã„ãã¾ã§ã®æ™‚é–“ãŒæ—©ã„ã€‚

## è¨ˆç®—å¼

å­¦ç¿’ç‡ï¼š$\varepsilon$ \
æ…£æ€§ï¼š$\mu$

$$
\begin{align}
V_t &= \mu V_{t-1} - \varepsilon \nabla E \\[8px]
w^{(t+1)} &= w^{(t)} + V_t
\end{align}
$$

## ã‚³ãƒ¼ãƒ‰

```
lr            å­¦ç¿’ç‡ï¼ˆlearning rateï¼‰ï¼ˆå°æ•°ï¼‰
params        å­¦ç¿’ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆè¡Œåˆ—ï¼‰
grads         ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é…ï¼ˆè¡Œåˆ—ï¼‰
momentum      æ…£æ€§é …ã®å¼·ã•ã‚’æ±ºã‚ã‚‹ä¿‚æ•°ï¼ˆ0ï½1ï¼‰
v             å‰å›ã®é‡ã¿
```

```python:python
class Momentum:
    def __init__(self, lr=0.01, momentum=0.9):
        self.lr = lr
        self.momentum = momentum
        self.v = None

    def update(self, params, grads):
        # åˆå›ã®åˆæœŸå€¤è¨­å®š
        if self.v is None:
            self.v = {}
            for key, val in params.items():
                self.v[key] = np.zeros_like(val)

        for key in params.keys():
            # å¼ï¼ˆï¼‘ï¼‰ã®å‡¦ç†
            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]
            # å¼ï¼ˆï¼’ï¼‰ã®å‡¦ç†
            params[key] += self.v[key]
```

# AdaGrad

## ç‰¹å¾´

- èª¤å·®ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å¾®åˆ†ã—ãŸã‚‚ã®ã¨å†å®šç¾©ã—ãŸå­¦ç¿’ç‡ã®ç©ã‚’æ¸›ç®—ã™ã‚‹ã€‚

- ãƒ¡ãƒªãƒƒãƒˆ
  - å‹¾é…ã®ç·©ã‚„ã‹ãªæ–œé¢ã«å¯¾ã—ã¦ã€æœ€é©å€¤ã«è¿‘ã¥ã‘ã‚‹ã€‚
- ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ
  - å­¦ç¿’ç‡ãŒå¾ã€…ã«å°ã•ããªã‚‹ã®ã§ã€éç‚¹å•é¡Œã‚’å¼•ãèµ·ã“ã™ï¼ˆå±€æ‰€æœ€é©è§£ã‹ã‚‰æŠœã‘å‡ºã›ãªã„ï¼‰äº‹ãŒã‚ã£ãŸã€‚

## è¨ˆç®—å¼

ä»»æ„ãªå€¤:$\theta$

$$
\begin{align}
  h_0 &= \theta \\[8px]
  h_t &= h_{t-1} + (\nabla E)^2 \\[8px]
  w^{(t+1)} &= w^{(t)} - \varepsilon \frac{1}{\sqrt{h_t}+\theta} \nabla E
\end{align}
$$

## ã‚³ãƒ¼ãƒ‰

```
lr            å­¦ç¿’ç‡ï¼ˆlearning rateï¼‰ï¼ˆå°æ•°ï¼‰
params        å­¦ç¿’ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆè¡Œåˆ—ï¼‰
grads         ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é…ï¼ˆè¡Œåˆ—ï¼‰
h             å­¦ç¿’ã®ãŸã³ã«å‹¾é…ã®2ä¹—ãšã¤å¢—åŠ ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¯ã®å€¤ï¼ˆè¡Œåˆ—ï¼‰
```

```python:python
class AdaGrad:
    def __init__(self, lr=0.01):
        self.lr = lr
        self.h = None

    def update(self, params, grads):
        # åˆå›ã®åˆæœŸå€¤è¨­å®š
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                # å¼ï¼ˆï¼“ï¼‰ã®å‡¦ç†
                self.h[key] = np.zeros_like(val)

        for key in params.keys():
            # å¼ï¼ˆï¼”ï¼‰ã®å‡¦ç†
            self.h[key] += grads[key] * grads[key]
            # å¼ï¼ˆï¼•ï¼‰ã®å‡¦ç†
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)
```

# RMSrop

## ç‰¹å¾´

- èª¤å·®ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å¾®åˆ†ã—ãŸã‚‚ã®ã¨å†å®šç¾©ã—ãŸå­¦ç¿’ç‡ã®ç©ã‚’æ¸›ç®—ã™ã‚‹ã€‚
- AdaGrad ã‚ˆã‚Šã‚‚æœ€è¿‘ã®å‹¾é…ã»ã©å¼·ãå½±éŸ¿ã™ã‚‹ã€‚

- ãƒ¡ãƒªãƒƒãƒˆ
  - å±€æ‰€çš„æœ€é©è§£ã«ã¯ãªã‚‰ãšã€å¤§åŸŸçš„æœ€é©è§£ã¨ãªã‚‹ã€‚
  - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´ãŒå¿…è¦ãªå ´åˆãŒå°‘ãªã„

## è¨ˆç®—å¼

0 ï½ 1 ã®å€¤ã§æ˜”ã®å‹¾é…æƒ…å ±ã‚’ã©ã®ç¨‹åº¦ä½¿ã†ã‹ï¼š$\alpha$ \
$\alpha$ãŒå°ã•ããªã‚‹ã¨ã€æ˜”ã®å‹¾é…æƒ…å ±ã‚’ç„¡è¦–ã™ã‚‹

$$
\begin{align}
  h_t &= \alpha h_{t-1} + (1 -\alpha)(\nabla E)^2 \\[8px]
  w^{(t+1)} &= w^{(t)} - \varepsilon \frac{1}{\sqrt{h_t}+\theta} \nabla E
\end{align}
$$

## ã‚³ãƒ¼ãƒ‰

```
lr            å­¦ç¿’ç‡ï¼ˆlearning rateï¼‰ï¼ˆå°æ•°ï¼‰
decay_rate    0 ï½ 1 ã®å€¤ã§æ˜”ã®å‹¾é…æƒ…å ±ã‚’ã©ã®ç¨‹åº¦ä½¿ã†ã‹
params        å­¦ç¿’ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆè¡Œåˆ—ï¼‰
grads         ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é…ï¼ˆè¡Œåˆ—ï¼‰
h             å­¦ç¿’ã®ãŸã³ã«å‹¾é…ã®2ä¹—ãšã¤å¢—åŠ ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¯ã®å€¤ï¼ˆè¡Œåˆ—ï¼‰
```

```python:pthon
class RMSprop:
    def __init__(self, lr=0.01, decay_rate = 0.99):
        self.lr = lr
        self.decay_rate = decay_rate
        self.h = None

    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)

        for key in params.keys():
            # å¼ï¼ˆï¼–ï¼‰ã®å‡¦ç†
            self.h[key] *= self.decay_rate
            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]
            # å¼ï¼ˆï¼—ï¼‰ã®å‡¦ç†
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)
```

# Adam

## ç‰¹å¾´

- ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ã®ã€éå»ã®å‹¾é…ã®æŒ‡æ•°é–¢æ•°çš„æ¸›è¡°å¹³å‡
- RMSProp ã®ã€éå»ã®å‹¾é…ã® 2 ä¹—ã®æŒ‡æ•°é–¢æ•°çš„æ¸›è¡°å¹³å‡
  ä¸Šè¨˜ã‚’ãã‚Œãã‚Œå­•ã‚“ã æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã‚ã‚‹ã€‚

- ãƒ¡ãƒªãƒƒãƒˆ
  - ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãŠã‚ˆã³ RMSProp ã®ãƒ¡ãƒªãƒƒãƒˆã‚’å­•ã‚“ã ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã‚ã‚‹ã€‚

## è¨ˆç®—å¼

$$
\begin{align}
  \nu_{t} &= \beta_1\nu_{t-1} + (1-\beta_1)G \\
  s_{t} &= \beta_2 s_{t-1} + (1-\beta_2)G^2 \\
  w_t &= w_{t-1} - \alpha\frac{\nu_{t}}{\sqrt{s_t + \epsilon}}
\end{align}
$$

## ã‚³ãƒ¼ãƒ‰

```python:python
class Adam:
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.iter = 0
        self.m = None
        self.v = None

    def update(self, params, grads):
        if self.m is None:
            self.m, self.v = {}, {}
            for key, val in params.items():
                self.m[key] = np.zeros_like(val)
                self.v[key] = np.zeros_like(val)

        self.iter += 1
        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)

        for key in params.keys():
            # å¼ï¼ˆï¼˜ï¼‰ã®å‡¦ç† (ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ )
            self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]
            unbias_m += (1 - self.beta1) * (grads[key] - self.m[key])
            # å¼ï¼ˆï¼™ï¼‰ã®å‡¦ç† (RMSrop)
            self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)
            unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key])
            # å¼ï¼ˆï¼‘ï¼ï¼‰ã®å‡¦ç†
            params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)
```

# å‚è€ƒ

[ã€æ±ºå®šç‰ˆã€‘ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚ã‹ã‚Šã‚„ã™ã„æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  -æå¤±é–¢æ•°ã‹ã‚‰ Adam ã¨ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ³æ³•-](https://qiita.com/omiita/items/1735c1d048fe5f611f80)
[[AI å…¥é–€] ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®ä»•çµ„ã¿ã€€ï½ãã® 4ï¼šæœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ¯”è¼ƒã—ã¦ã¿ãŸï½](https://tech-lab.sios.jp/archives/21823)

ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã¯[ã‚¼ãƒ­ã‹ã‚‰ä½œã‚‹ Deep Learning](https://www.oreilly.co.jp/books/9784873117584/)ã‹ã‚‰å¼•ç”¨
