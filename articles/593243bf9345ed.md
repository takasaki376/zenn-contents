---
title: "Deep Learningè³‡æ ¼è©¦é¨“ æ·±å±¤å­¦ç¿’ é †ä¼æ’­ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"
emoji: "ğŸ˜Š"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["Eè³‡æ ¼"]
published: true
---

# ã¯ã˜ã‚ã«

æ—¥æœ¬ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°å”ä¼šã® Deep Learning è³‡æ ¼è©¦é¨“ï¼ˆE è³‡æ ¼ï¼‰ã®å—é¨“ã«å‘ã‘ã¦ã€èª¿ã¹ãŸå†…å®¹ã‚’ã¾ã¨ã‚ã¦ã„ãã¾ã™ã€‚

# ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å…¨ä½“åƒ

æ·±å±¤å­¦ç¿’ã®ç™ºå±•ã¯ã€äººé–“ã®ç¥çµŒå›è·¯ç¶²ã‚’æ¨¡ã—ãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã†ã„æ•°ç†ãƒ¢ãƒ‡ãƒ«ãŒåŸºã¨ãªã£ã¦ãŠã‚Šã€è§£æ±ºã™ã¹ãå•é¡Œã«å¿œã˜ã¦ã•ã¾ã–ã¾ãªãƒ¢ãƒ‡ãƒ«ãŒææ¡ˆã•ã‚Œã¦ã„ã‚‹ã€‚

## å…¥åŠ›å±¤

ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…¨ä½“ã¨ã—ã¦ã€å…¥åŠ›å±¤ã‹ã‚‰ä¸­é–“å±¤ã«ãƒ‡ãƒ¼ã‚¿ãŒå¤‰æ›ã•ã‚ŒãªãŒã‚‰æ¸¡ã•ã‚Œã¦ã€æ´»æ€§åŒ–é–¢æ•°ã‚’é€šã—ã¦å‡ºåŠ›å±¤ã¸ãƒ‡ãƒ¼ã‚¿ãŒå‡ºåŠ›ã•ã‚Œã‚‹ã€‚

å…¥åŠ›å±¤ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ä½•ã‹ã—ã‚‰ã®æ•°å€¤ã‚’æ¸¡ã•ã‚Œã‚‹ã€‚å—ã‘å–ã‚‹å ´æ‰€ã‚’ãƒãƒ¼ãƒ‰ã¨ã„ã†ã€‚å…¥åŠ›å±¤ã‹ã‚‰ä¸­é–“å±¤ã¸æ¸¡ã™ã¨ãã«é‡ã¿ã‚’æº–å‚™ã™ã‚‹ã€‚å…¥åŠ›ãƒãƒ¼ãƒ‰æ¯ã«é‡è¦ãªé …ç›®ã§ã‚ã‚Œã°é‡ã¿ãŒå¤§ãããªã‚Šã€é‡è¦ã§ãªã„é …ç›®ã®é‡ã¿ã¯å°ã•ããªã‚‹ã€‚é‡ã¿ã ã‘ã§è¡¨ç¾ã§ããªã„å ´åˆã«ãƒã‚¤ã‚¢ã‚¹ã§èª¿æ•´ã™ã‚‹ã€‚

### å…¥åŠ›å±¤ã®è¨­è¨ˆ

- å…¥åŠ›å±¤ã¨ã—ã¦å–ã‚‹ã¹ãã§ãªã„ãƒ‡ãƒ¼ã‚¿
  - æ¬ æå€¤ãŒå¤šã„ãƒ‡ãƒ¼ã‚¿
  - èª¤å·®ã®å¤§ãã„ãƒ‡ãƒ¼ã‚¿
  - å‡ºåŠ›ãã®ã‚‚ã®ã€å‡ºåŠ›ã‚’åŠ å·¥ã—ãŸæƒ…å ±
  - é€£ç¶šæ€§ã®ãªã„ãƒ‡ãƒ¼ã‚¿ï¼ˆèƒŒç•ªå·ã¨ã‹ï¼‰
  - ç„¡æ„å‘³ãªæ•°ãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿
- æ¬ æå€¤ã®æ‰±ã„
  - ã‚¼ãƒ­ã§è©°ã‚ã‚‹
  - æ¬ æå€¤ã‚’å«ã‚€é›†åˆã‚’é™¤å¤–ã™ã‚‹
  - å…¥åŠ›ã¨ã—ã¦æ¡ç”¨ã—ãªã„
- ãƒ‡ãƒ¼ã‚¿ã®çµåˆ
- æ•°å€¤ã®æ­£è¦åŒ–ãƒ»æ­£å‰‡åŒ–

## ä¸­é–“å±¤ï¼ˆéš ã‚Œå±¤ï¼‰

ä¸­é–“å±¤ã®å‡ºåŠ›ã¯ã€å…¥åŠ›å±¤ã®è¨ˆç®—çµæœï¼ˆç·å…¥åŠ›ï¼‰ã«å¯¾ã—ã¦ã€æ´»æ€§åŒ–é–¢æ•°ã‚’é€šã™ã¨å¾—ã‚‰ã‚Œã‚‹ã€‚ä¸­é–“å±¤ã®ã®å‡ºåŠ›ã¯æ¬¡ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…¥åŠ›å€¤ã¨ãªã‚‹ã€‚

<!-- ## å‡ºåŠ›å±¤ -->

# æ´»æ€§åŒ–é–¢æ•°

## ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°

- 0 ï½ 1 ã®é–“ã‚’ç·©ã‚„ã‹ã«å¤‰åŒ–ã™ã‚‹é–¢æ•°ã§ã‚ã‚‹ã€‚
- èª²é¡Œã¨ã—ã¦ã€å¤§ããªå€¤ã§ã¯å‡ºåŠ›ã®å¤‰åŒ–ãŒå¾®å°ãªãŸã‚ã€å‹¾é…æ¶ˆå¤±å•é¡Œã‚’å¼•ãèµ·ã“ã™äº‹ãŒã‚ã‚‹ã€‚
- å¾®åˆ†ã™ã‚‹ã¨æœ€å¤§å€¤ã¯ 0.25 ã§ã‚ã‚‹ã€‚ï¼ˆ$x=0$ã®å ´åˆï¼‰

$$
h(x) = \frac {1}{1+e^{-1}}
$$

- å¾®åˆ†ã™ã‚‹ã¨ä¸‹è¨˜ã®è¨ˆç®—ã¨ãªã‚‹ã€‚

$$
 f'(x) = f(x)(1-f(x))
$$

ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã¯[ã‚¼ãƒ­ã‹ã‚‰ä½œã‚‹ Deep Learning](https://www.oreilly.co.jp/books/9784873117584/)ã‹ã‚‰å¼•ç”¨

```python:python
# ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã®å°é–¢æ•°
def sigmoid_grad(x):
    return (1.0 - sigmoid(x)) * sigmoid(x)
```

## ReLU é–¢æ•°

- å…¥åŠ›ãŒï¼ã‚’è¶…ãˆã¦ã„ã‚Œã°ã€ãã®å…¥åŠ›ã‚’ãã®ã¾ã¾å‡ºåŠ›ã—ã€ï¼ä»¥ä¸‹ãªã‚‰ã°ï¼ã‚’å‡ºåŠ›ã™ã‚‹é–¢æ•°ã§ã‚ã‚‹ã€‚
- å‹¾é…æ¶ˆå¤±å•é¡Œã®å›é¿ã¨ã‚¹ãƒ‘ãƒ¼ã‚¹åŒ–ã«è²¢çŒ®ã™ã‚‹ã“ã¨ã§è‰¯ã„æˆæœã‚’ã‚‚ãŸã‚‰ã—ã¦ã„ã‚‹ã€‚

$$
\begin{aligned}
    f(x) =
    \begin{cases}
        0 \quad (x \leqq 0) \\
        x \quad (x > 0) \\
    \end{cases}
\end{aligned}
$$

- å¾®åˆ†ã™ã‚‹ã¨ä¸‹è¨˜ã®çµæœã¨ãªã‚‹ã€‚

$$
\begin{aligned}
    f'(x) =
    \begin{cases}
        0 \quad (x \leqq 0) \\
        1 \quad (x > 0) \\
    \end{cases}
\end{aligned}
$$

### ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰

ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã¯[ã‚¼ãƒ­ã‹ã‚‰ä½œã‚‹ Deep Learning](https://www.oreilly.co.jp/books/9784873117584/)ã‹ã‚‰å¼•ç”¨

```python:python
# ReLUé–¢æ•°
def relu(x):
    return np.maximum(0, x)

# ReLUé–¢æ•°ã®å°é–¢æ•°
def relu_grad(x):
    grad = np.zeros_like(x)
    grad[x>=0] = 1
    return grad
```

## ãƒã‚¤ãƒ‘ãƒœãƒªãƒƒã‚¯ã‚¿ãƒ³ã‚¸ã‚§ãƒ³ãƒˆï¼ˆtanhï¼‰é–¢æ•°

- -1.0 ï½ 1.0 ã®ç¯„å›²ã®æ•°å€¤ã«å¤‰æ›ã—ã¦å‡ºåŠ›ã™ã‚‹é–¢æ•°ã§ã‚ã‚‹ã€‚
- åº§æ¨™ç‚¹(0, 0)ã‚’åŸºç‚¹ï¼ˆå¤‰æ›²ç‚¹ï¼‰ã¨ã—ã¦ç‚¹å¯¾ç§°ã§ã‚ã‚‹ã€‚
- å¾®åˆ†ã™ã‚‹ã¨æœ€å¤§å€¤ã¯ 1.0 ã§ã‚ã‚‹ã€‚ï¼ˆ$x=0$ã®å ´åˆï¼‰

$$
\begin{aligned}
    f(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}
\end{aligned}
$$

- å¾®åˆ†ã™ã‚‹ã¨ä¸‹è¨˜ã®è¨ˆç®—ã¨ãªã‚‹ã€‚

$$
 f'(x) = 1-f(x)^2
$$

### ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰

ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã¯[ã‚¼ãƒ­ã‹ã‚‰ä½œã‚‹ Deep Learning](https://www.oreilly.co.jp/books/9784873117584/)ã‹ã‚‰å¼•ç”¨

```python:python
# tanhé–¢æ•°
def tanh(x):
    return np.tanh(x)
```

## æ’ç­‰å†™åƒï¼ˆå›å¸°ï¼‰

$$
\begin{aligned}
  f(u) = u
\end{aligned}
$$

## ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•°ï¼ˆå¤šã‚¯ãƒ©ã‚¹åˆ†é¡ï¼‰

$$
\begin{aligned}
  f(i , u) = \frac{e^{u_i}}{\sum_{k=1}^K e^{u_K}}
\end{aligned}
$$

- å¾®åˆ†ã™ã‚‹ã¨ä¸‹è¨˜ã®è¨ˆç®—ã¨ãªã‚‹ã€‚

$$
\begin{aligned}
  \frac{\partial}{\partial u} f(i , u) = f(i , u) (\delta_{ik} - f(k , u))
\end{aligned}
$$

ã“ã“ã§ã€$\delta_{ik}$ã¯ã€ä¸‹è¨˜ã®å€¤ã‚’ã¨ã‚‹è¨˜å·ã§ã‚ã‚‹ã€‚

$$
\begin{aligned}
  \delta_{ik} =
    \begin{cases}
        0 \quad (i \neq k) \\
        1 \quad (i = k) \\
    \end{cases}
\end{aligned}
$$

ã¤ã¾ã‚Šã€$i = k$ã®æ™‚ã®å¾®åˆ†ã¯ã€

$$
\begin{aligned}
  \frac{\partial}{\partial u} f(i , u) = f(i , u) - f(i , u)^2
\end{aligned}
$$

$i \neq k$ã®æ™‚ã®å¾®åˆ†ã¯ã€

$$
\begin{aligned}
  \frac{\partial}{\partial u} f(i , u) = f(i , u)  f(k , u)
\end{aligned}
$$

### ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼å¯¾ç­–ã«ã¤ã„ã¦

ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•°ã«ã¯ã€å…¥åŠ›ã®å„è¦ç´ ã™ã¹ã¦ã«åŒã˜ã‚¹ã‚«ãƒ©ãƒ¼ã‚’åŠ ãˆã¦ã‚‚ã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•°ã®å‡ºåŠ›ã¯å¤‰ã‚ã‚‰ãªã„ã€‚

$$
\begin{aligned}
  softmax(z) = softmax(z + c)
\end{aligned}
$$

ã“ã®æ€§è³ªã¯ã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•°ã®åˆ†å­ã¨åˆ†æ¯ã«ä»»æ„ã®å®šæ•°ã‚’ã‹ã‘ã‚‹ã“ã¨ã§ã€å°ãã“ã¨ãŒã§ãã‚‹ã€‚ã“ã®æ€§è³ªã‹ã‚‰ã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•°ã¯å…¥åŠ›ã®å·®ã®ã¿ã«ä¾å­˜ã™ã‚‹ã€‚

$$
\begin{aligned}
  \frac{softmax(z)_i}{softmax(z)_j} = exp(z_i - z_j)
\end{aligned}
$$

ã“ã®æ€§è³ªã‚’ç”¨ã„ã‚‹ã¨ã€ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼å¯¾ç­–ã¨ã—ã¦ã€å…¥åŠ›$z$ã‹ã‚‰å…¥åŠ›å€¤ã®æœ€å¤§å€¤ã‚’æ¸›ã˜ã‚‹ã“ã¨ã§ã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•°ãŒæ•°å€¤çš„ã«å®‰å®šã™ã‚‹ã€‚

### ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰

ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã¯[ã‚¼ãƒ­ã‹ã‚‰ä½œã‚‹ Deep Learning](https://www.oreilly.co.jp/books/9784873117584/)ã‹ã‚‰å¼•ç”¨

```python:python
import numpy as np

def softmax(x):
    x = x - np.max(x, axis=-1, keepdims=True)   # ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼å¯¾ç­–
    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)
```

ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•°ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«é©ç”¨ã™ã‚‹ã“ã¨ã‚’è€ƒæ…®ã—ã¦ã€é †ä¼æ’­å‡¦ç†ã¨é€†ä¼æ’­å‡¦ç†ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹ã‚’ Softmax ã‚¯ãƒ©ã‚¹ã¨ã—ã¦å®Ÿè£…ã™ã‚‹ã¨ã€ä¸‹è¨˜ã®ã‚ˆã†ã«ãªã‚‹ã€‚

```python:python
class Softmax():
    def  __init__(self):
        self.params , self.grads = [], []
        self.out = None

    def forward(self, x):
        self.out =  = softmax(x)
        return self.out

    def backward(self, dout):
        dx = self.out * dout
        sumdx = np.sum(dx, axis=1, keepdims=True)
        dx -= self.out * sumdx
        return dx
```

# èª¤å·®é€†ä¼æ’­æ³•ï¼ˆãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰

## èª¤å·®å‹¾é…ã®è¨ˆç®—

$$
\begin{aligned}
  \nabla =  \frac{\partial E}{\partial w} = \left[ \frac{\partial E}{\partial w_1} \ldots \frac{\partial E}{\partial w_M}  \right] \\
\end{aligned}
$$

ã©ã†è¨ˆç®—ã™ã‚‹ã‹ï¼Ÿ

ç®—å‡ºã•ã‚ŒãŸèª¤å·®ã‚’ã€å‡ºåŠ›å±¤å´ã‹ã‚‰é †ã«å¾®åˆ†ã—ã€å‰ã®å±¤å‰ã®å±¤ã¸ã¨ä¼æ’­ã™ã‚‹ã€‚
æœ€å°é™ã®è¨ˆç®—ã§å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã®å¾®åˆ†å€¤ã‚’è§£æçš„ã«è¨ˆç®—ã™ã‚‹æ‰‹æ³•ã€‚

è¨ˆç®—çµæœï¼ˆ=èª¤å·®ï¼‰ã‹ã‚‰å¾®åˆ†ã‚’é€†ç®—ã™ã‚‹ã“ã¨ã§ã€ä¸è¦ãªå†å¸°çš„è¨ˆç®—ã‚’é¿ã‘ã¦å¾®åˆ†ã‚’ç®—å‡ºã§ãã‚‹ã€‚

## èª¤å·®ä¼æ¬æ³•ã¯ã©ã®ã‚ˆã†ã«è¨ˆç®—ã—ã¦ã„ã‚‹

ä¸‹è¨˜ã®å®šç¾©ãŒã‚ã‚‹å ´åˆã«ã€$w^{(2)}$ã‚’æ±‚ã‚ãŸã„ã€‚
ã¤ã¾ã‚Šã€$\frac{\partial E}{\partial w_{ji}^{(2)}}$ãŒæœ€çµ‚çš„ã«æ±‚ã‚ãŸã„å€¤ã§ã‚ã‚‹ã€‚

$$
\begin{aligned}
  E(y) &= \frac{1}{2} \sum_{j=1}^J (y_j - d_j)^2 = \frac{1}{2} ||y-d||^2  \\[8px]
  y &= u^{(L)}  \\[8px]
  u^l &= w^{(l)} z^{(l-1)} + b^{(l)}
\end{aligned}
$$

â€»$z$ã¯å‰ã®å±¤ã®å‡ºåŠ›

$\frac{\partial E}{\partial w_{ji}^{(2)}}$ã‚’å±•é–‹ã—ã¦ã„ã

$$
\begin{aligned}
  \frac{\partial E}{\partial w_{ji}^{(2)}} &= \frac{\partial E}{\partial y} \frac{\partial y}{\partial u} \frac{\partial u}{\partial w_{ji}^{(2)}} \\[8px]
  \frac{\partial E(y)}{\partial y} &= \frac{\partial}{\partial y} \frac{1}{2} ||y-d||^2 = y-b \\[8px]
  \frac{\partial y(u)}{\partial u} &= \frac{\partial u}{\partial u} = 1 \\[8px]
  \frac{\partial E}{\partial w_{ji}} &= \frac{\partial}{\partial w_{ji}}\left( w^{(l)} z^{(l-1)} + b^{(l)} \right) \\[8px]
  &= \frac{\partial}{\partial w_{ji}} \left( \left[
    \left[
      \begin{array}{ccccccccc}
        w_{11}z_1 & + & \ldots & + & w_{1i}z_i & + & \ldots & + & w_{1I}z_I \\
        & & & & \vdots & & & & & \\
        w_{j1}z_1 & + & \ldots & + & w_{ji}z_i & + & \ldots & + & w_{jI}z_I \\
        & & & & \vdots & & & & & \\
        w_{J1}z_1 & + & \ldots & + & w_{Ji}z_i & + & \ldots & + & w_{JI}z_I \\
      \end{array}
    \right] +
    \left[
      \begin{array}{c}
        b_1 \\
        \vdots \\
        b_j \\
        \vdots \\
        b_J
      \end{array}
    \right]
  \right] \right)
  = \left[
    \begin{array}{c}
      0 \\
      \vdots \\
      z_i \\
      \vdots \\
      0
    \end{array}
  \right] \\[8px]
\end{aligned}
$$

$E$ ã‚’ $w_{ji}$ã§å¾®åˆ†ã™ã‚‹ãŸã‚ã€$w$ã®è¡Œåˆ—ã®çœŸã‚“ä¸­ã®å€¤ä»¥å¤–ã¯å¾®åˆ†ã—ãŸçµæœã€ï¼ã«ãªã‚‹ã€‚

$$
\begin{aligned}
\frac{\partial E}{\partial y} \frac{\partial y}{\partial u} \frac{\partial u}{\partial w_{ji}^{(2)}} &= (y-d) \cdot  \left[
    \begin{array}{c}
      0 \\
      \vdots \\
      z_i \\
      \vdots \\
      0
    \end{array}
\right] = (y_j - d_j)z_i
\end{aligned}
$$

# èª¤å·®é–¢æ•°

$y_i$ã¯äºˆæ¸¬å€¤ã€$t_i$ã¯çœŸã®å€¤

## äºŒä¹—å’Œèª¤å·®

$$
\begin{aligned}
E_n(w) = \frac{1}{2} \sum_{i=1}^I (y_i - t_i)^2
\end{aligned}
$$

### å¾®åˆ†ã™ã‚‹ã¨

$$
\begin{aligned}
  \frac{\partial E_n(w)}{\partial y_i} = y_i - t_i
\end{aligned}
$$

## äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼

$$
\begin{aligned}
E_n(w) = - \sum_{i=1}^I t_i \log{y_i}
\end{aligned}
$$

### å¾®åˆ†ã™ã‚‹ã¨

$$
\begin{aligned}
  \frac{\partial E_n(w)}{\partial y_i} = - \frac{t_i}{y_i}
\end{aligned}
$$

### ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰

ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã¯[ã‚¼ãƒ­ã‹ã‚‰ä½œã‚‹ Deep Learning](https://www.oreilly.co.jp/books/9784873117584/)ã‹ã‚‰å¼•ç”¨

```python:python
def cross_entropy_error(y, t):
    """
    y : ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•°ã®å‡ºåŠ›
    t : æ­£è§£ãƒ©ãƒ™ãƒ«
    """
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)

    # æ•™å¸«ãƒ‡ãƒ¼ã‚¿ãŒone-hot-vectorã®å ´åˆã€æ­£è§£ãƒ©ãƒ™ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ›
    if t.size == y.size:
        t = t.argmax(axis=1)

    batch_size = y.shape[0]
    delta = 1e-7
    return -np.sum(np.log(y[np.arange(batch_size), t] + delta)) / batch_size

class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None
        self.y = None # softmaxã®å‡ºåŠ›
        self.t = None # æ•™å¸«ãƒ‡ãƒ¼ã‚¿

    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy_error(self.y, self.t)

        return self.loss

    def backward(self, dout=1):
        batch_size = self.t.shape[0]
        if self.t.size == self.y.size: # æ•™å¸«ãƒ‡ãƒ¼ã‚¿ãŒone-hot-vectorã®å ´åˆ
            dx = (self.y - self.t) / batch_size
        else:
            dx = self.y.copy()
            dx[np.arange(batch_size), self.t] -= 1
            dx = dx / batch_size

        return dx
```
