---
title: "Deep Learning資格試験 まとめ"
emoji: "🦁"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["E資格"]
published: false
---

# はじめに

日本ディープラーニング協会の Deep Learning 資格試験（E 資格）の受験に向けて、調べた内容をまとめていきます。

# 応用数学

## [線形代数](https://zenn.dev/takasaki/articles/4122b2256de856)

- 演算規則
  - 和
  - スカラー倍
  - 行列式
  - 単位行列
  - 逆行列
- 連立方程式
  - 行基本変形
- 固有値分解
  - 固有値、固有ベクトル
- 特異値分解
- いろいろな距離
  - マンハッタン距離
  - ユークリッド距離
  - マハラノビス距離

## [確率・統計（１）](https://zenn.dev/takasaki/articles/9d8e6e24afcbc0)

- 集合
  - 和集合
  - 共通部分
  - 絶対補
  - 相対補
- 確率
  - 頻度確率
  - ベイズ確率
  - 条件付き確率
  - 独立な事象の同時確率
  - ベイズ則
- 期待値
- 分散・共分散
  - 分散
  - 共分散
  - 標準偏差
- 確率変数と確率分布
- さまざまな分布

## [確率・統計（２）](https://zenn.dev/takasaki/articles/8e56fdf82d9a24)

- 統計的推定
  - 最尤推定

## [情報理論](https://zenn.dev/takasaki/articles/34bb62f480d590)

- 自己情報量
- 平均情報量
- 結合エントロピー
- 条件付きエントロピー
- 相互情報量
- 相対エントロピー
- 交差エントロピー

# [機械学習（１）](https://zenn.dev/takasaki/articles/5b093114e53fad)

- 学習アルゴリズム
  - タスクＴ
  - 性能指標Ｐ
  - 経験Ｅ
- 前処理
  - 欠損処理
  - 外れ値除去
  - 正規化
  - 標準化
  - アンダーサンプリング
  - ダミー変数化（one hot encoding）
  - バッグオブワーズ
  - 特徴選択
- モデルの学習方法
  - ホールドアウト法
  - クロスバリデーション（K-分割交差検証）
- 機械学習の性能評価
  - 未学習
  - 過学習
  - バイアス-バリアンス分解

# [機械学習（２）](https://zenn.dev/takasaki/articles/b7a4215e759262)

- ハイパーパラメータ探索
  - グリッドサーチ
  - ランダムサーチ
  - ベイズ最適化
- 正則化（パラメータノルムペナルティー）
  - L2 正則化（リッジ回帰）
  - L1 正則化（ラッソ回帰）
  - エラスティックネット

## [機械学習モデル](https://zenn.dev/takasaki/articles/ceecd45894715d)

- 線形回帰・非線形回帰
- ロジスティック回帰
- Ｋ近傍法
- SVM
- K-means
- 主成分分析

## [評価関数](https://zenn.dev/takasaki/articles/3a3e221fba1822)

- 混同行列
- 正解率（accuracy）
- 適合率（precision）
- 再現率（recall）
- Ｆ値（F-measure）
- 特異度（specificity）
- 偽陽性率 (False Positive Rate、FPR)
- ROC 曲線
- 平均絶対誤差（Mean Absolute Error 、MAE）
- 平均二乗誤差（Mean Squared Error 、MSE）
- 平均二乗誤差平方根（Root Mean Squared Error 、RMSE）

# 深層学習

## [順伝播ネットワーク](https://zenn.dev/takasaki/articles/593243bf9345ed)

- ニューラルネットワーク全体像
  - 入力層
  - 中間層（隠れ層）
  - 出力層
- 活性化関数
  - シグモイド関数
  - ReLU 関数
  - ハイパボリックタンジェント（tanh）関数
- 誤差逆伝搬法

## [最適化アルゴリズム](https://zenn.dev/takasaki/articles/771d507e9eff3f)

- 勾配降下法
- 確率的勾配降下法
- ミニバッチ勾配降下法
- モメンタム
- AdaGrad
- RMSrop
- Adam

## [正則化・最適化](https://zenn.dev/takasaki/articles/ae05efaa7d6915)

- 最適化戦略
  - バッチ正規化
  - Layer 正規化
  - Instance 正規化
  - ドロップアウト

## [畳み込みニューラルネットワーク](https://zenn.dev/takasaki/articles/f7c99060d621a7)

- 畳み込み層
- プーリング層
- ストライド
- パディング
- Attention
- データセット
- 評価関数
  - IoU
  - AP
  - mAP

## [CNN 代表的なモデル](https://zenn.dev/takasaki/articles/d5743f1de5d7d0)

- 画像分類モデル
  - VGG
  - GoogleNet
  - ResNet
  - Dense Net
- 物体検出モデル
  - YOLO
  - SDD
  - MobileNet
  - R-CNN
  - Faster R-CNN

## [再帰的ニューラルネットワーク](https://zenn.dev/takasaki/articles/ce60ffcd6f736a)

- 概要
- BPTT
- 制度指標
  - BLEU
- 代表的なモデル
  - LSTM
  - GRU
  - 双方向 RNN
  - Seq2Seq
  - HERD
  - VHRED
  - VAE
  - Word2vec

## [強化学習](https://zenn.dev/takasaki/articles/1703e40d4a66fc)

- 概要
  - 探索と利用のトレードオフ
  - 強化学習のイメージ
  - 強化学習と通常の教師あり、教師なし学習との違い
  - 強化学習の歴史
  - 価値関数
  - 方策関数
- 代表的なモデル
  - GAN
  - DQN
  - Pix2Pix

## [生成モデル](https://zenn.dev/takasaki/articles/fee1a02310003b)

- 代表的なモデル
  - WaveNet

## [Transformer](https://zenn.dev/takasaki/articles/81ce21b8a22290)

- Transformer の仕組み
  - Transformer の概要
  - Attention は何をしているのか
  - Transformer の長所
  - Transformer の短所
  - 位置エンコーディング
  - 注意機構
  - 注意機構には二種類ある
    - ソース・ターゲット注意機構（Source Target Attention）
    - 自己注意機構（Self-Attention）
  - Position-Wise Feed-Forward Networks
  - Scaled dot product attention
  - Multi-Head attention
- 自然言語処理への応用（BERT）
  - BERT の概要
  - 事前学習とファインチューニング
  - 順伝搬層の活性化関数
  - BERT の入力と埋め込み層
  - マスク化言語モデル（MLM）
  - 事前学習
    - Feature-based
    - Fine-tuning
  - ファインチューニング
- 画像処理への応用（ViT）
  - ViT の概要
  - 埋め込み層
  - ViT の Transformer ブロック
  - 出力層
- 音声認識への応用（Conformer）
  - Conformer の概要
  - SpecAugment
  - Convolution Subsampling
  - Dropout
  - Feed Forward Module
  - Multi-Head Self-Attention Module
  - Convolution module
  - Pointwise Convolution
  - 1 次元 Depthwise Convolution
  - Gated Linear Unit（GLU）
  - Batch Normalization（バッチ正規化）
