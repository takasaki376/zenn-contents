---
title: "Deep Learning資格試験 まとめ"
emoji: "🦁"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["E資格"]
published: true
---

# はじめに

日本ディープラーニング協会の Deep Learning 資格試験（E 資格）の受験に向けて、調べた内容をまとめていきます。

# 応用数学

## 線形代数

@[card](https://zenn.dev/takasaki/articles/4122b2256de856)

- 演算規則
  - 和
  - スカラー倍
  - 行列式
  - 単位行列
  - 逆行列
- 連立方程式
  - 行基本変形
- 固有値分解
  - 固有値、固有ベクトル
- 特異値分解
- いろいろな距離
  - マンハッタン距離
  - ユークリッド距離
  - マハラノビス距離

## 確率・統計（１）

@[card](https://zenn.dev/takasaki/articles/9d8e6e24afcbc0)

- 集合
  - 和集合
  - 共通部分
  - 絶対補
  - 相対補
- 確率
  - 頻度確率
  - ベイズ確率
  - 条件付き確率
  - 独立な事象の同時確率
  - ベイズ則
- 期待値
- 分散・共分散
  - 分散
  - 共分散
  - 標準偏差
- 確率変数と確率分布
- さまざまな分布

## 確率・統計（２）

@[card](https://zenn.dev/takasaki/articles/8e56fdf82d9a24)

- 統計的推定
  - 最尤推定

## 情報理論

@[card](https://zenn.dev/takasaki/articles/34bb62f480d590)

- 自己情報量
- 平均情報量
- 結合エントロピー
- 条件付きエントロピー
- 相互情報量
- 相対エントロピー
- 交差エントロピー

# 機械学習

## 機械学習（１）

@[card](https://zenn.dev/takasaki/articles/5b093114e53fad)

- 学習アルゴリズム
  - タスクＴ
  - 性能指標Ｐ
  - 経験Ｅ
- 前処理
  - 欠損処理
  - 外れ値除去
  - 正規化
  - 標準化
  - アンダーサンプリング
  - ダミー変数化（one hot encoding）
  - バッグオブワーズ
  - 特徴選択
- モデルの学習方法
  - ホールドアウト法
  - クロスバリデーション（K-分割交差検証）
- 機械学習の性能評価
  - 未学習
  - 過学習
  - バイアス-バリアンス分解

## 機械学習（２）

@[card](https://zenn.dev/takasaki/articles/b7a4215e759262)

- ハイパーパラメータ探索
  - グリッドサーチ
  - ランダムサーチ
  - ベイズ最適化
- 正則化（パラメータノルムペナルティー）
  - L2 正則化（リッジ回帰）
  - L1 正則化（ラッソ回帰）

## 機械学習モデル

@[card](https://zenn.dev/takasaki/articles/ceecd45894715d)

- 線形回帰・非線形回帰
- ロジスティック回帰
- Ｋ近傍法
- SVM
- K-means
- 主成分分析

## 評価関数

@[card](https://zenn.dev/takasaki/articles/3a3e221fba1822)

- 混同行列
- 正解率（accuracy）
- 適合率（precision）
- 再現率（recall）
- Ｆ値（F-measure）
- 特異度（specificity）
- 偽陽性率 (False Positive Rate、FPR)
- ROC 曲線
- 平均絶対誤差（Mean Absolute Error 、MAE）
- 平均二乗誤差（Mean Squared Error 、MSE）
- 平均二乗誤差平方根（Root Mean Squared Error 、RMSE）
- 物体認識、物体検出で使用される評価指標
  - IoU
  - AP
  - mAP

# 深層学習

## 深層学習概要

@[card](https://zenn.dev/takasaki/articles/593243bf9345ed)

- ニューラルネットワーク全体像
  - 入力層
  - 中間層（隠れ層）
  <!-- - 出力層 -->
- 活性化関数
  - シグモイド関数
  - ReLU 関数
  - ハイパボリックタンジェント（tanh）関数
- 誤差逆伝搬法

## 最適化アルゴリズム

@[card](https://zenn.dev/takasaki/articles/771d507e9eff3f)

- 勾配降下法
- 確率的勾配降下法（SDG）
- ミニバッチ勾配降下法
- モメンタム
- AdaGrad
- RMSrop
- Adam

## 最適化・高速化・軽量化

@[card](https://zenn.dev/takasaki/articles/ae05efaa7d6915)

- 初期値の設定方法
  - 通常
  - Xavier
  - He
- ドロップアウト
- Weight decay(荷重減衰)
- 最適化戦略
  - バッチ正規化
  - Layer 正規化
  - Instance 正規化
- 高速化
  - データ並列化
  - モデル並列化
- 軽量化
  - 量子化
  - 蒸留
  - プルーニング

## 畳み込みニューラルネットワーク

@[card](https://zenn.dev/takasaki/articles/f7c99060d621a7)

- レイヤー
  - 畳み込み層
    - ストライド
    - パディング
  - プーリング層
  - ダイレクト畳み込み
  - 転置畳み込み(transposed convolution)、逆畳み込み(Deconvolution)
- 畳み込み層の出力画像サイズの計算
- 物体検出
- セマンティックセグメンテーション
- 転移学習
- データ拡張(Data Augmentation)
- データセット
- im2col

## CNN 代表的なモデル

@[card](https://zenn.dev/takasaki/articles/d5743f1de5d7d0)

- 物体認識モデル
  - VGG
  - GoogleNet
  - ResNet
  - DenseNet
- 物体検出モデル
  - MobileNet
  - R-CNN
  - Faster R-CNN
  - YOLO
  - SSD
- セマンティックセグメンテーション
  - FCN(Fully Convolutional Network)
  - SegNet
  - U-Net

## 再帰的ニューラルネットワーク

@[card](https://zenn.dev/takasaki/articles/ce60ffcd6f736a)

- 概要
- BPTT
- 制度指標
  - BLEU
- 自然言語処理
  - Word2vec

## RNN 代表的なモデル

- 時系列データ
  - LSTM
  - GRU
  - 双方向 RNN
- 系列変換モデル
  - Seq2Seq
  - HERD
  - VHRED
  - VAE

## 強化学習

@[card](https://zenn.dev/takasaki/articles/1703e40d4a66fc)

- 概要
  - 探索と利用のトレードオフ
  - 強化学習のイメージ
  - 強化学習と通常の教師あり、教師なし学習との違い
  - 強化学習の歴史
  - 価値関数
  - 方策関数

## 強化学習 代表的なモデル

@[card](https://zenn.dev/takasaki/articles/b576d716e185c2)

- DQN
- DCGAN

## 生成モデル

@[card](https://zenn.dev/takasaki/articles/fee1a02310003b)

- GAN
- WaveNet
- Pix2Pix

## Transformer

@[card](https://zenn.dev/takasaki/articles/81ce21b8a22290)

- Transformer の仕組み
  - Transformer の概要
  - Attention は何をしているのか
  - Transformer の長所
  - Transformer の短所
  - 位置エンコーディング
  - 注意機構
  - 注意機構には二種類ある
    - ソース・ターゲット注意機構（Source Target Attention）
    - 自己注意機構（Self-Attention）
  - Position-Wise Feed-Forward Networks
  - Scaled dot product attention（スケール・ドットプロダクト・アテンション）
  - Multi-Head attention（マルチヘッド・アテンション）
- 自然言語処理への応用（BERT）
  - BERT の概要
  - 事前学習とファインチューニング
  - 順伝搬層の活性化関数
  - BERT の入力と埋め込み層
  - マスク化言語モデル（MLM）
  - 事前学習
    - Feature-based
    - Fine-tuning
  - ファインチューニング
- 画像処理への応用（ViT）
  - ViT の概要
  - 埋め込み層
  - ViT の Transformer ブロック
  - 出力層
- 音声認識への応用（Conformer）
  - Conformer の概要
  - SpecAugment
  - Convolution Subsampling
  - Dropout
  - Feed Forward Module
  - Multi-Head Self-Attention Module
  - Convolution module
  - Pointwise Convolution
  - 1 次元 Depthwise Convolution
  - Gated Linear Unit（GLU）
  - Batch Normalization（バッチ正規化）
