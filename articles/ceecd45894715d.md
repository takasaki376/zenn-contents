---
title: "Deep Learning資格試験 機械学習モデル"
emoji: "🎉"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["E資格"]
published: true
---

# はじめに

日本ディープラーニング協会の Deep Learning 資格試験（E 資格）の受験に向けて、調べた内容をまとめていきます。

# 線形回帰・非線形回帰

- 学習種類：教師あり学習
- タスク：予測問題
- パラメータ推定：最小２乗法・最尤推定

## 特徴

- 表現力の高いモデルに対して正則化すると、予測結果が滑らかになる。
- 説明変数間に相関があると、良い予測ができない可能性がある。
- 一般にモデルに入れる説明変数の数が多いほど、表現力が上がる。
- ラッソ正則化において正則化係数を十分に大きくすると、いくつかの係数は完全に０になる。

# ロジスティック回帰

- 学習種類：教師あり学習
- タスク：分類問題
- パラメータ推定：最尤推定

### 尤度関数

尤度関数が最大となるようにパラメータを学習する。

$$
\begin{aligned}
  \prod_n \left( p(y_n=1 | x_n)^{y_n} p(y_n = 0 | x_n)^{1-y_n} \right)
\end{aligned}
$$

- 尤度関数は、データ数が多いとき、尤度は桁が非常に小さくなり、アンダーフローを起こす。
- 尤度は積で表現されているため、勾配を求める際に面倒くさい。

### 対数尤度関数

$$
\begin{aligned}
  \sum_n \left( y_n \log p(y_n=1 | x_n) + (1 - y_n) \log p(y_n = 0 | x_n) \right)
\end{aligned}
$$

$p(y_n = 1 | x_n)$は、出力$\hat{y}$であることから、$p(y_n = 0 | x_n)$は、$1 - \hat{y}$と書くことができる。
そのため、全データに対する負の対数尤度は、

$$
\begin{aligned}
  - \sum_n \left( y_n \log \hat{y} + (1 - y_n) \log (1 - \hat{y}))\right)
\end{aligned}
$$

と書くことができる。

### パラメータの解釈

学習で得られたパラメータの解釈にはオッズが用いられる。
オッズは、

$$
\begin{aligned}
  \frac{p(y = 1 | x)}{p(y = 0 | x)} = \frac{\hat{y}}{1 - \hat{y}}
\end{aligned}
$$

を使用する。オッズの計算方法は、ロジスティック回帰の出力

$$
\begin{aligned}
  \hat{y} = \frac{1}{1 + exp(-w^T x -b)}
\end{aligned}
$$

を代入する。

$$
\begin{aligned}
  \frac{p(y = 1 | x)}{p(y = 0 | x)} &= \frac{\hat{y}}{1 - \hat{y}} \\[12px]
  &= \frac{1}{1 + exp(-w^T x - b)} \div \left( 1 - \frac{1}{1 + exp(-w^T x -b)} \right) \\[12px]
  &= \frac{1}{1 + exp(-w^T x - b)} \div \left( \frac{1 + exp(-w^T x - b)}{1 + exp(-w^T x - b)} - \frac{1}{1 + exp(-w^T x - b)} \right) \\[12px]
  &= \frac{1}{1 + exp(-w^T x - b)} \times \frac{1 + exp(-w^T x - b)}{exp(-w^T x - b)} \\[12px]
  &= \frac{1}{exp(-w^T x - b)} \\[12px]
  &= exp(w^T x + b)
\end{aligned}
$$

# Ｋ近傍法

- 学習種類：教師あり学習
- タスク：分類問題

# SVM

- 学習種類：教師あり学習
- タスク：分類問題
- パラメータ推定：マージン最大化
- ２クラス分類

## 概要

- 分類境界を挟んで２つのクラスがどのくらい離れているかを`マージン`と呼ぶ。
- マージンが最大となる境界を学習する。
- マージン上のデータ点を`サポートベクトル`と呼ぶ。
- 境界線の計算式の結果が正負でクラスを判断する。

## ハードマージン

- 訓練データを完璧に分類できる決定関数が存在するという前提で分類する。

## ソフトマージン

- 多少の誤りを許容する。

## カーネルトリック

- 非線形問題を扱う場合、射影関数$\phi$を使って、線形分離できないデータを高次元の特徴空間に変換し、線形分離可能とする。
- 次元を増やすと計算量が莫大になるため、カーネルトリックを使って計算コストを削減できる。

### 多項式カーネル

$$
K(x_i,x_j) = \left[ x_i^Tx_j + c \right]^d
$$

### ガウスカーネル

$$
K(x_i,x_j) = exp(-\gamma ||x_i-x_j||^2)
$$

### シグモイドカーネル

$$
K(x_i,x_j) = tanh(bx_i^Tx_j +c)
$$

# k-means

- 学習種類：教師なし学習
- タスク：クラスタリング
- クラスタ数は、はじめに決めておく必要がある。

## k-means++

- k-means では初めのセントロイドの位置決めをランダムで行うため、うまく分類できない場合があった
- 各セントロイド同士の距離がなるべく遠くなるように位置を決めるようにした。

# 主成分分析

- 学習種類：教師なし学習
- タスク：次元削減
- パラメータ推定：分散最大化
