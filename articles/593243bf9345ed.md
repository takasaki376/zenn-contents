---
title: "Deep Learning資格試験 深層学習 順伝播ネットワーク"
emoji: "😊"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["E資格"]
published: false
---

# はじめに

日本ディープラーニング協会の Deep Learning 資格試験（E 資格）の受験に向けて、調べた内容をまとめていきます。

# ニューラルネットワーク全体像

## 入力層

## 中間層（隠れ層）

## 出力層

# 活性化関数

## シグモイド関数

- 0 ～ 1 の間を緩やかに変化する関数である。
- 課題として、大きな値では出力の変化が微小なため、勾配消失問題を引き起こす事がある。
- 微分すると最大値は 0.25 である。（$x=0$の場合）

$$
h(x) = \frac {1}{1+e^{-1}}
$$

- 微分すると下記の計算となる。

$$
 f'(x) = f(x)(1-f(x))
$$

[ゼロから作る Deep Learning](https://www.oreilly.co.jp/books/9784873117584/)から引用

```python:python
# シグモイド関数
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# シグモイド関数の導関数
def sigmoid_grad(x):
    return (1.0 - sigmoid(x)) * sigmoid(x)
```

## ReLU 関数

- 入力が０を超えていれば、その入力をそのまま出力し、０以下ならば０を出力する関数である。
- 勾配消失問題の回避とスパース化に貢献することで良い成果をもたらしている。

$$
\begin{aligned}
    f(x) =
    \begin{cases}
        0 \quad (x \leqq 0) \\
        x \quad (x > 0) \\
    \end{cases}
\end{aligned}
$$

- 微分すると下記の結果となる。

$$
\begin{aligned}
    f(x) =
    \begin{cases}
        0 \quad (x \leqq 0) \\
        1 \quad (x > 0) \\
    \end{cases}
\end{aligned}
$$

[ゼロから作る Deep Learning](https://www.oreilly.co.jp/books/9784873117584/)から引用

```python:python
# ReLU関数
def relu(x):
    return np.maximum(0, x)

# ReLU関数の導関数
def relu_grad(x):
    grad = np.zeros_like(x)
    grad[x>=0] = 1
    return grad
```

## ハイパボリックタンジェント（tanh）関数

- -1.0 ～ 1.0 の範囲の数値に変換して出力する関数である。
- 座標点(0, 0)を基点（変曲点）として点対称である。
- 微分すると最大値は 1.0 である。（$x=0$の場合）

$$
\begin{aligned}
    f(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}
\end{aligned}
$$

- 微分すると下記の計算となる。

$$
 f'(x) = 1-f(x)^2
$$

[ゼロから作る Deep Learning](https://www.oreilly.co.jp/books/9784873117584/)から引用

```python:python
# tanh関数
def tanh(x):
    return np.tanh(x)
```

# 誤差逆伝搬法
