---
title: "Deep Learning資格試験 応用数学 情報理論"
emoji: "👻"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["E資格"]
published: false
---

# Stage1 応用数学 Section3 情報理論

## 自己情報量

ある事象$x$の発生確率が$P(x)$である時の情報量のこと。
$W(x)$：事象が起きた時、それが$x$である数みたいなもの。

発生確率が低い方(珍しい)が情報量が多い。
また、情報量は加法性がある。

- 対数の底がの底が２の時、単位はビット（bit）
- 対数ネイピアの$e$の時、単位は（nat）

$$
  l(x) = - \log {(P(x)} = \log{(w(x)) }
$$

参考：[自己情報量とは？分かりやすく解説します！](https://www.krrk0.com/amount-of-self-information/)

## 平均情報量（シャノンエントロピー）

- 自己情報量の期待値
- 事象$x$の平均情報量。

予測できなさ、不確定さとも言える。
例）コイントスの場合、表ばかり出る、裏ばかり出るよりも表裏が大体同じくらい出る(次どっちでるか分からない)方がこのシャノンエントロピーは大きくなる。

$$
\begin{aligned}
  H(x) &= E(I(x)) \\[12px]
     &= - E(\log (P(x)) \\[12px]
     &= - \sum _{} ^{} {P(x) \log(p(x))}
\end{aligned}
$$

参考：[平均情報量とは？計算方法を分かりやすく解説！](https://www.krrk0.com/average-amount-of-information/)

## 結合エントロピー

参考：[結合エントロピーって何？分かりやすく解説しました！](https://www.krrk0.com/joint-entropy/)

## 条件付きエントロピー

参考：[条件付きエントロピーとは？簡単に解説！](https://www.krrk0.com/conditional-entropy/)

## 相互情報量

参考：[相互情報量とは？簡単に解説してみました！](https://www.krrk0.com/mutual-information/)

## カルバック・ライブラー　ダイバージェンス（ＫＬダイバージェンス）

- 同じ事象・確率変数における異なる確率分布 $P$,$Q$がどれだけ似ているかを表す。
- 完全に同じ場合は０、違いが大きくなると大きな値になる。
- マイナスにならない。

KL 情報量、KL 距離とも呼ぶ。
確率$P$、確率$Q$の確率分布がどれだけ近いか、どれだけ遠いか距離のように表す。
確率$Q$だったと思ってたら確率$P$だと判明した時、どれくらい違うか。
そのため P から Q、Q から P で見た時、値が変わる。

$$
\begin{aligned}
  D_{kL}( P \parallel Q ) &= E_{x～P} \log{\frac{P(x)}{Q(x)}} \\[12px]
  &= E_{x～P} {[\log(P(x)) - \log(Q(x))]} \\[12px]
  I(Q(x))-I(P(x)) &= (-log(Q(x)))-(-log(P(x)))\\[12px]
  &=log\frac{P(x)}{Q(x)} \\[12px]
  D_{KL}(P\parallel{Q}) &= \sum_{x}{P(x)(-log(Q(x)))-(-log(P(x)))} \\
  &=\sum_{x}{P(x)\frac{P(x)}{Q(x)}}
\end{aligned}
$$

## 交差エントロピー

- KL ダイバージェンスの一部分を取り出したもの。
- $Q$についての自己情報量を$P$の分布で平均している。

$$
\begin{aligned}
  H(P,Q) &= H(P) + D_{KL}(P \parallel Q) \\[6px]
  H(P,Q) &= -E_{x～P} \log{Q(x)}
\end{aligned}
$$
