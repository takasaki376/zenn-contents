---
title: 'Deep Learning資格試験 深層学習 最適化アルゴリズム'
emoji: '💨'
type: 'tech' # tech: 技術記事 / idea: アイデア
topics: ['E資格']
published: false
---

# はじめに

日本ディープラーニング協会の Deep Learning 資格試験（E 資格）の受験に向けて、調べた内容をまとめていきます。

# 勾配降下法

## 特徴

- 目的関数が最小となるパラメータを勾配表によって探す。
- 目的関数が各パラメータによって微分できる必要がある。

- メリット
  - データが冗⻑な場合の計算コストの軽減
  - 望まない局所極小解に収束するリスクの軽減
  - オンライン学習ができる

## 計算式

学習率：$\varepsilon$

$$
\begin{aligned}
w^{(t+1)} = w^{(t)} - \varepsilon \nabla E
\end{aligned}
$$

## コード

```
steps         学習回数（整数）
parameter     学習するパラメータ（行列）
grad          パラメータの勾配（行列）
lr            学習率（learning rate）（小数）
sqrt(x)       xの平方根
```

```python:python
for i in range(steps):
    parameter = parameter - lr * grad
```

# 確率的勾配降下法

- 無作為に選び出されたデータを用いて行う勾配降下法である。

# ミニバッチ勾配降下法

- 無作為に選び出された複数のデータを用いて行う勾配降下法である。

# モメンタム

## 特徴

- 誤差をパラメータで微分したものと学習率の積を減算した後、現在の重みに前回の重みを減算した値と慣性の積を加算する

- メリット
  - 局所的最適解にはならず、大域的最適解となる。
  - 谷間についてから最も低い位置(最適値)にいくまでの時間が早い。

## 計算式

学習率：$\varepsilon$ \
慣性：$\mu$

$$
\begin{aligned}
V_t &= \mu V_{t-1} - \varepsilon \nabla E \\[8px]
w^{(t+1)} &= w^{(t)} - V_t
\end{aligned}
$$

## コード

```python:python
v=0                  #gradと同じサイズの行列
for i in range(steps):
    v = v * momentum - lr * grad
    parameter = parameter + v
```

# AdaGrad

## 特徴

- 誤差をパラメータで微分したものと再定義した学習率の積を減算する

- メリット
  - 勾配の緩やかな斜面に対して、最適値に近づける。
- デメリット
  - 学習率が徐々に小さくなるので、鞍点問題を引き起こす事があった。

## 計算式

任意な値:$\theta$

$$
\begin{aligned}
  h_0 &= \theta \\[8px]
  h_t &= h_{t-1} + (\nabla E)^2 \\[8px]
  w^{(t+1)} &= w^{(t)} - \varepsilon \frac{1}{\sqrt{h_t}+\theta} \nabla E
\end{aligned}
$$

## コード

```python:python
h=0                     #gradと同じサイズの行列
for i in range(steps):
    h = h + grad * grad
    parameter = parameter - lr * grad / (sqrt(h) + epsilon)
```

# RMSrop

## 特徴

- 誤差をパラメータで微分したものと再定義した学習率の積を減算する。
- AdaGrad よりも最近の勾配ほど強く影響する。

- メリット
  - 局所的最適解にはならず、大域的最適解となる。
  - ハイパーパラメータの調整が必要な場合が少ない

## 計算式

0 ～ 1 の値で昔の勾配情報をどの程度使うか：$\alpha$ \
$\alpha$が小さくなると、昔の勾配情報を無視する

$$
\begin{aligned}
  h_t &= \alpha h_{t-1} + (1 -\alpha)(\nabla E)^2 \\[8px]
  w^{(t+1)} &= w^{(t)} - \varepsilon \frac{1}{\sqrt{h_t}+\theta} \nabla E
\end{aligned}
$$

## コード

```python:pthon
h=0                  #gradと同じサイズの行列
for i in range(steps):
    h = rho * h + (1 - rho) * grad * grad
    parameter = parameter - lr * grad / (sqrt(h) + epsilon)
```

# Adam

## 特徴

- モメンタムの、過去の勾配の指数関数的減衰平均
- RMSProp の、過去の勾配の 2 乗の指数関数的減衰平均
  上記をそれぞれ孕んだ最適化アルゴリズムである。

- メリット
  - モメンタムおよび RMSProp のメリットを孕んだアルゴリズムである。

## 計算式

$$
\begin{aligned}
  \nu_{t} &= \beta_1\nu_{t-1} + (1-\beta_1)G \\
  s_{t} &= \beta_2s_{t-1} + (1-\beta_2)G^2 \\
  w_t &= w_{t-1} - \alpha\frac{\nu_{t}}{\sqrt{s_t + \epsilon}}
\end{aligned}
$$

## コード

```python:python
m=0                #gradと同じサイズの行列
v=0                #gradと同じサイズの行列
for i in range(steps):
    m = beta_1 * m + (1 - beta_1) * grad
    v = beta_2 * v + (1 - beta_2) * grad^2
    om = m / (1 - beta_1)
    ov = v / (1 - beta_2)
    parameter = parameter - lr * om / sqrt(ov + epsilon)
```

# 参考

[【決定版】スーパーわかりやすい最適化アルゴリズム -損失関数から Adam とニュートン法-](https://qiita.com/omiita/items/1735c1d048fe5f611f80)
[[AI 入門] ディープラーニングの仕組み　～その 4：最適化アルゴリズムを比較してみた～](https://tech-lab.sios.jp/archives/21823)
