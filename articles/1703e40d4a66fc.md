---
title: "Deep Learning資格試験 深層学習 強化学習"
emoji: "👻"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["E資格"]
published: false
---

# はじめに

日本ディープラーニング協会の Deep Learning 資格試験（E 資格）の受験に向けて、調べた内容をまとめていきます。

# 概要

- 長期的に報酬を最大化できるように環境のなかで行動を選択できるエージェントを作ることを目標とする機械学習の一分野
  - 行動の結果として与えられる利益(報酬)をもとに、行動を決定する原理を改善していく仕組みである。

## 探索と利用のトレードオフ

- 環境について事前に完璧な知識があれば、最適な行動を予測し決定することは可能。
  - どのような顧客にキャンペーンメールを送信すると、どのような行動を行うのかが既知である状況。
    - 強化学習の場合、上記仮定は成り立たないとする。不完全な知識を元に行動しながら、データを収集。最適な行動を見つけていく。

1. 過去のデータで、ベストとされる行動のみを常に取り続ければ他にもっとベストな行動を見つけることはできない。
   - 探索が足りない
1. 未知の行動のみを常に取り続ければ、過去の経験が活かせない。
   - 利用が足りない

`1`と`2`はトレードオフ

## 強化学習のイメージ

- エージェント
  - 方策：何をやったら価値が高くなるか考えて行動する。
- 環境
  - 状態：環境の状態で、状況に応じて変わる内容
- エージェント
  - 価値：行動の結果、報酬を得られる。

エージェントの方策として、良い方策を考えるのが強化学習

## 強化学習と通常の教師あり、教師なし学習との違い

結論:目標が違う

- 教師なし、あり学習では、データに含まれるパターンを見つけ出すおよびそのデータから予測することが目標
- 強化学習では、優れた方策を見つけることが目標

### 強化学習の歴史

#### 強化学習について

- 冬の時代があったが、計算速度の進展により大規模な状態をもつ場合の、強化学習を可能としつつある。
- 関数近似法と、Q 学習を組み合わせる手法の登場

#### Q 学習

- 行動価値関数を、行動する毎に更新することにより学習を進める方法

#### 関数近似法

- 価値関数や方策関数を関数近似する手法のこと

## 価値関数

- 価値関数とは
  - 価値を表す関数としては、状態価値関数と行動価値関数の 2 種類がある

1. 状態価値関数：ある状態の価値に注目する
1. 行動価値関数：状態と価値を組み合わせた価値に注目する

## 方策関数

- 方策関数とはエージェントが、どんな行動をするのかを決める関数である。
- 方策ベースの強化学習手法において、ある状態でどのような行動を採るのかの確率を与える関数のこと。
- 価値関数の結果を最大化するように学習する。

- $\pi_{\theta}(a|s)$：エージェントが取る行動の確率(方策関数)
- $V^{\pi}(s)$：ある状態でから得られる報酬(状態価値関数)
- $Q^{\pi}(s,a)$：ある状態で取ったある行動から得られる報酬(行動価値関数)
- $\pi_{\theta}(a|s)Q^{\pi}(s,a)$：ある行動をとる時の報酬

### 方策反復法

- 方策をモデル化して最適化する手法 -方策勾配法

$$
\begin{aligned}
\theta^{(t+1)} = \theta^{(t)} + \epsilon \nabla J(\theta)
\end{aligned}
$$

$t$：時間
$\theta$：重み
$\epsilon$：学習率
$J$：誤差関数

- $J$とは、方策の良さ
  - 定義しなければならない

### 方策方程式

- 定義方法
  - 平均報酬
  - 割引報酬和

上記の定義に対応して、行動価値関数:Q(s,a)の定義を行い、方策勾配定理が成り立つ。

$$
\begin{aligned}
  \nabla_{\theta} J(\theta) =\mathbb{E}_{\pi_{\theta}}[(\nabla_{\theta} log\pi_{\theta} (a|s) Q^{\pi}(s,a))]
\end{aligned}
$$

- $\pi_{\theta}(a|s)$：エージェントが取る行動の確率(方策関数)
- $Q^{\pi}(s,a)$：ある状態で取ったある行動から得られる報酬(行動価値関数)
- $\pi_{\theta}(a|s)Q^{\pi}(s,a)$：ある行動をとる時の報

# Transformer

# 代表的なモデル

## GAN

## DQN

## DCGAN

- GAN を利用した画像生成モデル
- 中間層に全結合層を使わない
- バッチノーマライゼーションを適用する
- Generator
  - Pooling 層の代わりに転置畳み込み層を使用する。
  - 最終層は tanh、その他は ReLU 関数で活性化する。
- Discriminator
  - Pooling 層の代わりに畳み込み層を使用する。
  - Leaky ReLU 関数で活性化する。

## Pix2Pix

### タスク

- 線画を入力画像として、色つきの画像を出力する
- 一部がマスクされた画像を入力として、マスク部分が補完された画像を出力する
- 低画質な画像を入力として、高画質な画像を出力する
