---
title: "Deep Learning資格試験 深層学習 強化学習"
emoji: "👻"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["E資格"]
published: true
---

# はじめに

日本ディープラーニング協会の Deep Learning 資格試験（E 資格）の受験に向けて、調べた内容をまとめていきます。

# 概要

- 長期的に報酬を最大化できるように環境のなかで行動を選択できるエージェントを作ることを目標とする機械学習の一分野
  - 行動の結果として与えられる利益(報酬)をもとに、行動を決定する原理を改善していく仕組みである。

## 探索と利用のトレードオフ

- 環境について事前に完璧な知識があれば、最適な行動を予測し決定することは可能。
  - どのような顧客にキャンペーンメールを送信すると、どのような行動を行うのかが既知である状況。
    - 強化学習の場合、上記仮定は成り立たないとする。不完全な知識を元に行動しながら、データを収集。最適な行動を見つけていく。

1. 過去のデータで、ベストとされる行動のみを常に取り続ければ他にもっとベストな行動を見つけることはできない。
   - 探索が足りない
1. 未知の行動のみを常に取り続ければ、過去の経験が活かせない。
   - 利用が足りない

`1`と`2`はトレードオフ

## 強化学習のイメージ

- エージェント
  - 方策：何をやったら価値が高くなるか考えて行動する。
- 環境
  - 状態：環境の状態で、状況に応じて変わる内容
- エージェント
  - 価値：行動の結果、報酬を得られる。

エージェントの方策として、良い方策を考えるのが強化学習

## 強化学習と通常の教師あり、教師なし学習との違い

結論:目標が違う

- 教師なし、あり学習では、データに含まれるパターンを見つけ出すおよびそのデータから予測することが目標
- 強化学習では、優れた方策を見つけることが目標

### 強化学習の歴史

#### 強化学習について

- 冬の時代があったが、計算速度の進展により大規模な状態をもつ場合の、強化学習を可能としつつある。
- 関数近似法と、Q 学習を組み合わせる手法の登場

#### Q 学習

- 行動価値関数を、行動する毎に更新することにより学習を進める方法

#### 関数近似法

- 価値関数や方策関数を関数近似する手法のこと

## 価値関数

- 価値関数とは
  - 価値を表す関数としては、状態価値関数と行動価値関数の 2 種類がある

1. 状態価値関数：ある状態の価値に注目する
1. 行動価値関数：状態と価値を組み合わせた価値に注目する

### マルコフ決定過程（MDP）

- 参考
  [【強化学習】マルコフ決定過程（MDP）の原理・例題・計算方法](https://algorithm.joho.info/machine-learning/markov-decision-process/)
  [強化学習 2 　マルコフ決定過程・ベルマン方程式](https://qiita.com/ngayope330/items/3466b1260c93cac202fb)

- 次に起こる事象の確率が、これまでの過程と関係なく、現在の状態によってのみ決定される確率過程のこと
- 以前の状態に依存しない性質のことを「マルコフ性」という。

### ベルマン方程式

- 参考
  [【JDLA E 資格】強化学習の価値関数とベルマン方程式](https://qiita.com/fridericusgauss/items/a2b868490eb809b19872)
  [今さら聞けない強化学習（1）：状態価値関数と Bellman 方程式](https://qiita.com/triwave33/items/5e13e03d4d76b71bc802)

## 方策関数

- 方策関数とはエージェントが、どんな行動をするのかを決める関数である。
- 方策ベースの強化学習手法において、ある状態でどのような行動を採るのかの確率を与える関数のこと。
- 価値関数の結果を最大化するように学習する。

- $\pi_{\theta}(a|s)$：エージェントが取る行動の確率(方策関数)
- $V^{\pi}(s)$：ある状態でから得られる報酬(状態価値関数)
- $Q^{\pi}(s,a)$：ある状態で取ったある行動から得られる報酬(行動価値関数)
- $\pi_{\theta}(a|s)Q^{\pi}(s,a)$：ある行動をとる時の報酬

### 方策反復法

- 方策をモデル化して最適化する手法 -方策勾配法

$$
\begin{aligned}
\theta^{(t+1)} = \theta^{(t)} + \epsilon \nabla J(\theta)
\end{aligned}
$$

$t$：時間
$\theta$：重み
$\epsilon$：学習率
$J$：誤差関数

- $J$とは、方策の良さ
  - 定義しなければならない

### 方策方程式

- 定義方法
  - 平均報酬
  - 割引報酬和

上記の定義に対応して、行動価値関数:Q(s,a)の定義を行い、方策勾配定理が成り立つ。

$$
\begin{aligned}
  \nabla_{\theta} J(\theta) =\mathbb{E}_{\pi_{\theta}}[(\nabla_{\theta} log\pi_{\theta} (a|s) Q^{\pi}(s,a))]
\end{aligned}
$$

- $\pi_{\theta}(a|s)$：エージェントが取る行動の確率(方策関数)
- $Q^{\pi}(s,a)$：ある状態で取ったある行動から得られる報酬(行動価値関数)
- $\pi_{\theta}(a|s)Q^{\pi}(s,a)$：ある行動をとる時の報

# 強化学習の手法

## 動的計画法（DP 法）

- 環境の完全なモデルを必要とする。
- 環境の完全なモデルがマルコフ決定過程として与えられている場合に適応できる。（ただし、実際の問題で環境の完全なモデルで明らかなことはほとんどない。）
- 実際に適用できる手法で、方策反復法と価値反復法がある。

## モンテカルロ法（MC 法）

- 遷移のサンプルを取得し、得られた収益を平均化することによって、価値関数を推定する方法である。
- サンプルを数多く取得し、そこから平均を求めるという計算を行うため、「モンテカルロ」という名前がついている。
- 価値関数の改善または方策の改善は、エピソード終了後に行う。

## 時間的差分学習（TD 法）

- 目標の価値と現在の価値のずれを修正していくことにより、価値関数を推定する方法である。
- 目標の価値と現在の価値のずれのことを、TD 誤差という。
- 価値関数の改善または方策の改善は、エピソード終了をまたずに行う。
- TD 学習の具体的な手法として、Sarsa と Q 学習がある。
