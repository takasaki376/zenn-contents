---
title: "Deep Learning資格試験 応用数学 情報理論"
emoji: "👻"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["E資格"]
published: true
---

# はじめに

日本ディープラーニング協会の Deep Learning 資格試験（E 資格）の受験に向けて、調べた内容をまとめていきます。

# 自己情報量

- ある事象$x$の発生確率が$P(x)$である時の情報量のこと。
- $w(x)$：事象が起きた時、それが$x$である数みたいなもの。
- 発生確率が低い方(珍しい)が情報量が多い。
- 情報量は加法性がある。（2 つの事象が独立であれば、2 つの事象が同時に起こった時の情報量は、それぞれの情報量の和と等しくなる。）

- 対数の底がの底が２の時、単位はビット（bit）
- 対数ネイピアの$e$の時、単位は（nat）

$$
\begin{aligned}
  I(x) &= - \log {P(x)}
\end{aligned}
$$

参考：[自己情報量とは？分かりやすく解説します！](https://www.krrk0.com/amount-of-self-information/)

# 平均情報量（シャノンエントロピー）

- 自己情報量の期待値
- 事象$x$の平均情報量。

予測できなさ、不確定さとも言える。
例）コイントスの場合、表ばかり出る、裏ばかり出るよりも表裏が大体同じくらい出る(次どっちでるか分からない)方がこのシャノンエントロピーは大きくなる。

- 離散確率変数$X$において、$X=x$となる確率が$P(x)$のとき、確率変数$X$のエントロピーは次の式となる。

$$
  \begin{aligned}
    H(X) &= E(I(x)) \\[12px]
       &= - E \log P(x) \\[12px]
       &= - \sum _{} ^{} {P(x) \log P(x)}
  \end{aligned}
$$

参考：[平均情報量とは？計算方法を分かりやすく解説！](https://www.krrk0.com/average-amount-of-information/)

- 離散型確率分布$P$のエントロピーは、次の式となる。

$$
  \begin{aligned}
    H(X) &= - \int_x {P(x) \log P(x)} dx
  \end{aligned}
$$

# 結合エントロピー

- 「コインが表か裏か」を知ることで得られる平均情報量を$H(A)$、「サイコロの出た目」を知ることで得られる平均情報量を$H(B)$とする。
- コインとサイコロの結果は共通点がないため、２つの平均情報量を足すと、２つの情報を同時に知ったときに得られる情報量となる。

$$
\begin{aligned}
  H(A ,B) &= H(A) + H(B)
\end{aligned}
$$

- 「サイコロの出た目」が「3 以下か 4 以上か」を知ることで得られる平均情報量を$H(A)$、「偶数か奇数か」を知ることで得られる平均情報量を$H(B)$とする。
- ２つの情報には図のように「共通している部分」があるため、加法性がない。

|     |   1    |   2    |   3    |   4    |   5    |   6    |
| :-- | :----: | :----: | :----: | :----: | :----: | :----: |
| A   | 3 以下 | 3 以下 | 3 以下 | 4 以上 | 4 以上 | 4 以上 |
| B   |  奇数  |  偶数  |  奇数  |  偶数  |  奇数  |  偶数  |

$$
\begin{aligned}
  H(A ,B) &= H(A) + H(B \backslash A) \\
  &= H(B) + H(A \backslash B)
\end{aligned}
$$

参考：[結合エントロピーって何？分かりやすく解説しました！](https://www.krrk0.com/joint-entropy/)

# 条件付きエントロピー

- $B$の値は知っているという状態から、$A$を知ることで得られる平均情報量

$$
\begin{aligned}
  H(A \backslash B) = - \sum P(B) \sum P(A \backslash B) \log {(P(A \backslash B)})
\end{aligned}
$$

参考：[条件付きエントロピーとは？簡単に解説！](https://www.krrk0.com/conditional-entropy/)

# 相互情報量

- 「2 つの情報が互いにどれだけ影響し合っているか」を表すもの。
- 相互情報量は「平均情報量」と「条件付きエントロピー」の差である。

$$
\begin{aligned}
  I(A ,B) &= H(A) - H(A \backslash B) \\
  &= H(B) - H(B \backslash A)
\end{aligned}
$$

参考：[相互情報量とは？簡単に解説してみました！](https://www.krrk0.com/mutual-information/)

# 相対エントロピー（ＫＬダイバージェンス）

- 同じ事象・確率変数における異なる確率分布 $P$,$Q$がどれだけ似ているかを表す。
- 完全に同じ場合は０、違いが大きくなると大きな値になる。
- マイナスにならない。

KL 情報量、KL 距離とも呼ぶ。
確率$P$、確率$Q$の確率分布がどれだけ近いか、どれだけ遠いか距離のように表す。
確率$Q$だったと思ってたら確率$P$だと判明した時、どれくらい違うか。
そのため $P$ から $Q$、$Q$ から $P$ で見た時、値が変わる。

$$
\begin{aligned}
  D_{KL}( P \parallel Q ) &= E_{x～P} \left[ \log{\frac{P(x)}{Q(x)}} \right] \\[12px]
  &= E_{x～P} {[ \log P(x) - \log Q(x)]} \\[12px]
  I(Q(x))-I(P(x)) &= (-\log Q(x))-(-\log P(x))\\[12px]
  &=\log \frac{P(x)}{Q(x)} \\[12px]
  D_{KL}(P\parallel{Q}) &= \sum_{x}{P(x)(-\log Q(x))-(-\log P(x))} \\[12px]
  &= \sum_{x}{P(x) \log \frac{P(x)}{Q(x)}} \\[12px]
  &= - \sum_{x}{P(x) \log \frac{Q(x)}{P(x)}}
\end{aligned}
$$

- 二つの確率が全く同じ時に０となる。

$$
\begin{aligned}
  D_{KL}( P \parallel P ) &= \sum_{x}{P(x) \log \frac{P(x)}{P(x)}} \\[12px]
  &= \sum_{x}{P(x)} \log \hspace{1mm} 1 \\[6px]
  &= 0
\end{aligned}
$$

- 離散型確率分布$P$と$Q$のＫＬダイバージェンスは、次の式となる。

$$
  \begin{aligned}
    D_{KL}( P \parallel Q ) &= \int_x {P(x) \log \frac{P(x)}{Q(x)}} dx \\[12px]
    &= - \int_x {P(x) \log \frac{Q(x)}{P(x)}} dx
  \end{aligned}
$$

# 交差エントロピー（クロスエントロピー）

- KL ダイバージェンスの一部分を取り出したもの。
- $Q$についての自己情報量を$P$の分布で平均している。

$$
\begin{aligned}
  H(P,Q) &= H(P(x)) + D_{KL}(P(x) \parallel Q(x)) \\[6px]
  H(P,Q) &= -E_{x～P} \log{Q(x)} \\[6px]
  H(P,Q) &= -\sum_x P(x) \log Q(x)
\end{aligned}
$$

- 離散型確率分布$P$と$Q$の交差エントロピーは、次の式となる。

$$
\begin{aligned}
  H(P,Q) &= - \int_x {P(x) \log Q(x)} dx
\end{aligned}
$$
